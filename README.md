# Practice 4 & 5

## Description

В данном проекте реализовано:

1. Три семантических анализатора для поиска похожих по смыслу документов:
    -   **LSA**: Расширенная версия анализатора из ПР№2.
    -   **Doc2Vec**: Нейросетевой подход для векторизации документов.
    -   **RNN**: Нейросетевой подход на основе рекуррентных сетей для извлечения семантических векторов.

2. **Seq2Seq** Генератор текста на основе LSTM, который обучается на корпусе документов и генерирует осмысленный текст с использованием POS анализа.

## Installation

```bash
pip install -r requirements.txt
```

## Usage

Данные для обучения моделей (`20 Newsgroups`) автоматически скачиваются из библиотеки **scikit-learn** и кэшируются в директорию `train/`.

### LSA Analyser
```bash
python run_lsa.py <file_path>
```
**Example:**
```bash
python run_lsa.py doc.txt
```

### Doc2Vec Analyser
```bash
python run_doc2vec.py <file_path>
```
**Example:**
```bash
python run_doc2vec.py doc.txt
```

### RNN Analyser
```bash
python run_rnn.py <file_path>
```
**Example:**
```bash
python run_rnn.py doc.txt
```

### Seq2Seq Generator
```bash
python run_seq2seq.py <seed>
```
**Example:**
```bash
python run_seq2seq.py "hardware includes"
```


## Comparison

Семантический анализатор обладает следующим набором функций:

- **LSA** находит документы со схожим набором ключевых терминов, работая на уровне статистики слов. Использует TF-IDF для взвешивания слов и SVD для выявления скрытых тем.
- **Doc2Vec** находит документы, похожие по контексту и смыслу. Является расширением Word2Vec и строит векторные представления для целых документов, а не только слов.
- **RNN** обучается на задаче классификации текстов. После обучения один из внутренних слоев сети используется как векторизатор. ТАкой подход улавливает наиболее сложные зависимости, но требует тщательной настройки гиперпараметров.
- **Seq2Seq** обучается предсказывать следующее слово в данной последовательности, модель построена на нейросетевой архитектуре LSTM.

Для задач, требующих улавливания тонких смысловых связей, нейросетевые подходы (`Doc2Vec`, `RNN`) являются более предпочтительными.

Качество сгенерированного текста `Seq2Seq`-модели напрямую зависит от сложности её архитектуры и тонкой настройки гиперпараметров.